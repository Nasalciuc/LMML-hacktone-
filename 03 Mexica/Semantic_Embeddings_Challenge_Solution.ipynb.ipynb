{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6962c111-0dda-4436-9768-aa0b3fe16a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ MEGA-ADVANCED WORD2VEC PIPELINE INITIALIZED\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üîß FINAL SOLUTION SETUP\")\n",
    "print(\"=\" * 30)\n",
    "print(\"üìã Plan: PPMI+SVD deterministic approach\")\n",
    "print(\"üéØ Target: SIGMOID_ENGINEER_ATHLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "457b783d-36d8-4497-bc63-ec6e2f023108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ CORPUS LOADED\n",
      "üìä Original length: 5933 characters\n",
      "üìù Raw segments: 130\n",
      "‚úÖ PREPROCESSING COMPLETE:\n",
      "  üìù Processed sentences: 130\n",
      "  üî§ Total words: 1522\n",
      "  üìö Unique words: 229\n",
      "\n",
      "üîç First 3 processed sentences:\n",
      "  1. ['doctors', 'practice', 'medicine', 'daily', 'relying', 'on', 'medical', 'knowledge']...\n",
      "  2. ['medicine', 'forms', 'the', 'foundation', 'of', 'every', 'doctor', 's']...\n",
      "  3. ['doctors', 'dedicate', 'years', 'to', 'studying', 'medicine', 'before', 'practicing']...\n"
     ]
    }
   ],
   "source": [
    "# Citire corpus\n",
    "with open('corpus.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"üìñ CORPUS LOADED\")\n",
    "print(f\"üìä Original length: {len(text)} characters\")\n",
    "\n",
    "# Preprocessare \"de fier\"\n",
    "# 1. Segmentare √Æn propozi»õii\n",
    "sentences_raw = []\n",
    "for segment in text.split('.'):\n",
    "    segment = segment.strip()\n",
    "    if segment:\n",
    "        sentences_raw.append(segment)\n",
    "\n",
    "for segment in text.split('\\n'):\n",
    "    segment = segment.strip()\n",
    "    if segment and segment not in sentences_raw:\n",
    "        sentences_raw.append(segment)\n",
    "\n",
    "print(f\"üìù Raw segments: {len(sentences_raw)}\")\n",
    "\n",
    "# 2. Tokenizare strict [a-zA-Z]+ »ôi lowercase\n",
    "processed_sentences = []\n",
    "all_words = []\n",
    "\n",
    "for sentence in sentences_raw:\n",
    "    # Extrage doar cuvinte alfabetice\n",
    "    words = re.findall(r'[a-zA-Z]+', sentence.lower())\n",
    "    if len(words) >= 3:  # pƒÉstreazƒÉ doar propozi»õii substan»õiale\n",
    "        processed_sentences.append(words)\n",
    "        all_words.extend(words)\n",
    "\n",
    "print(f\"‚úÖ PREPROCESSING COMPLETE:\")\n",
    "print(f\"  üìù Processed sentences: {len(processed_sentences)}\")\n",
    "print(f\"  üî§ Total words: {len(all_words)}\")\n",
    "print(f\"  üìö Unique words: {len(set(all_words))}\")\n",
    "\n",
    "# Afi»ôez primele propozi»õii\n",
    "print(f\"\\nüîç First 3 processed sentences:\")\n",
    "for i, sent in enumerate(processed_sentences[:3], 1):\n",
    "    print(f\"  {i}. {sent[:8]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "21731e6e-39cc-4107-a720-8a5a2d841ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö VOCABULARY CONSTRUCTION\n",
      "  Size: 229 unique words\n",
      "\n",
      "üéØ Key words in vocabulary:\n",
      "  ‚úÖ doctors: 42 occurrences\n",
      "  ‚úÖ medicine: 44 occurrences\n",
      "  ‚úÖ law: 44 occurrences\n",
      "  ‚úÖ engineers: 42 occurrences\n",
      "  ‚úÖ teachers: 44 occurrences\n",
      "  ‚úÖ schools: 34 occurrences\n",
      "  ‚úÖ athletes: 44 occurrences\n",
      "  ‚úÖ hospitals: 36 occurrences\n",
      "\n",
      "üîó BUILDING CO-OCCURRENCE MATRIX\n",
      "  Matrix size: 229 x 229\n",
      "‚úÖ Co-occurrence matrix built\n",
      "  Non-zero entries: 3592\n"
     ]
    }
   ],
   "source": [
    "# Construire vocabular\n",
    "vocab = sorted(set(all_words))\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"üìö VOCABULARY CONSTRUCTION\")\n",
    "print(f\"  Size: {vocab_size} unique words\")\n",
    "\n",
    "# Verificare cuvinte cheie\n",
    "key_words = ['doctors', 'medicine', 'law', 'engineers', 'teachers', 'schools', 'athletes', 'hospitals']\n",
    "print(f\"\\nüéØ Key words in vocabulary:\")\n",
    "for word in key_words:\n",
    "    status = \"‚úÖ\" if word in word_to_idx else \"‚ùå\"\n",
    "    count = all_words.count(word)\n",
    "    print(f\"  {status} {word}: {count} occurrences\")\n",
    "\n",
    "# Construire matrice co-ocuren»õƒÉ\n",
    "print(f\"\\nüîó BUILDING CO-OCCURRENCE MATRIX\")\n",
    "print(f\"  Matrix size: {vocab_size} x {vocab_size}\")\n",
    "\n",
    "window = 4  # fereastrƒÉ simetricƒÉ\n",
    "cooc_matrix = np.zeros((vocab_size, vocab_size), dtype=np.float32)\n",
    "\n",
    "for sentence in processed_sentences:\n",
    "    for i, word1 in enumerate(sentence):\n",
    "        if word1 in word_to_idx:\n",
    "            idx1 = word_to_idx[word1]\n",
    "            \n",
    "            # FereastrƒÉ simetricƒÉ\n",
    "            start = max(0, i - window)\n",
    "            end = min(len(sentence), i + window + 1)\n",
    "            \n",
    "            for j in range(start, end):\n",
    "                if i != j and j < len(sentence):\n",
    "                    word2 = sentence[j]\n",
    "                    if word2 in word_to_idx:\n",
    "                        idx2 = word_to_idx[word2]\n",
    "                        cooc_matrix[idx1, idx2] += 1\n",
    "\n",
    "print(f\"‚úÖ Co-occurrence matrix built\")\n",
    "print(f\"  Non-zero entries: {np.count_nonzero(cooc_matrix)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d05f8fdb-5d95-4ffa-9e76-903f0d9366ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßÆ PPMI TRANSFORMATION\n",
      "=========================\n",
      "üìä Statistics:\n",
      "  Total co-occurrences: 9576\n",
      "  Average word frequency: 41.82\n",
      "‚úÖ PPMI transformation complete\n",
      "  PPMI non-zero entries: 3398\n",
      "  Max PPMI value: 5.4781\n"
     ]
    }
   ],
   "source": [
    "print(\"üßÆ PPMI TRANSFORMATION\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Calculez probabilitƒÉ»õi\n",
    "total_count = np.sum(cooc_matrix)\n",
    "word_counts = np.sum(cooc_matrix, axis=1)\n",
    "\n",
    "print(f\"üìä Statistics:\")\n",
    "print(f\"  Total co-occurrences: {int(total_count)}\")\n",
    "print(f\"  Average word frequency: {np.mean(word_counts):.2f}\")\n",
    "\n",
    "# PPMI transformation\n",
    "ppmi_matrix = np.zeros_like(cooc_matrix)\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    for j in range(vocab_size):\n",
    "        if cooc_matrix[i, j] > 0:\n",
    "            # p(i,j) = count(i,j) / total\n",
    "            p_ij = cooc_matrix[i, j] / total_count\n",
    "            \n",
    "            # p(i) = count(i) / total, p(j) = count(j) / total\n",
    "            p_i = word_counts[i] / total_count\n",
    "            p_j = word_counts[j] / total_count\n",
    "            \n",
    "            if p_i > 0 and p_j > 0:\n",
    "                # PMI = log(p(i,j) / (p(i) * p(j)))\n",
    "                pmi = np.log(p_ij / (p_i * p_j))\n",
    "                # PPMI = max(0, PMI)\n",
    "                ppmi_matrix[i, j] = max(0, pmi)\n",
    "\n",
    "print(f\"‚úÖ PPMI transformation complete\")\n",
    "print(f\"  PPMI non-zero entries: {np.count_nonzero(ppmi_matrix)}\")\n",
    "print(f\"  Max PPMI value: {np.max(ppmi_matrix):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e662468d-d198-48e2-a351-213e77909625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ SVD DECOMPOSITION\n",
      "=========================\n",
      "  Running SVD with 100 components...\n",
      "  Explained variance ratio: [0.00929592 0.04178572 0.03290763 0.03006608 0.02894421]\n",
      "  Cumulative variance: 0.8788\n",
      "\n",
      "üîß POST-PROCESSING VECTORS\n",
      "‚úÖ Embeddings ready:\n",
      "  Shape: (229, 100)\n",
      "  Vector norm range: [3.0358, 11.0192]\n",
      "  Test similarity doctors-medicine: 0.6260\n"
     ]
    }
   ],
   "source": [
    "print(\"üî¢ SVD DECOMPOSITION\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# SVD cu 100 componente\n",
    "n_components = min(100, vocab_size - 1)\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "\n",
    "print(f\"  Running SVD with {n_components} components...\")\n",
    "word_vectors = svd.fit_transform(ppmi_matrix)\n",
    "\n",
    "print(f\"  Explained variance ratio: {svd.explained_variance_ratio_[:5]}\")\n",
    "print(f\"  Cumulative variance: {np.sum(svd.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "# Post-procesare vectori\n",
    "print(f\"\\nüîß POST-PROCESSING VECTORS\")\n",
    "\n",
    "# 1. Mean centering\n",
    "mean_vector = np.mean(word_vectors, axis=0)\n",
    "word_vectors_centered = word_vectors - mean_vector\n",
    "\n",
    "# 2. L2 normalization\n",
    "norms = np.linalg.norm(word_vectors_centered, axis=1, keepdims=True)\n",
    "word_vectors_normalized = word_vectors_centered / (norms + 1e-8)\n",
    "\n",
    "print(f\"‚úÖ Embeddings ready:\")\n",
    "print(f\"  Shape: {word_vectors_normalized.shape}\")\n",
    "print(f\"  Vector norm range: [{np.min(norms):.4f}, {np.max(norms):.4f}]\")\n",
    "\n",
    "# Test similaritate\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2) + 1e-8)\n",
    "\n",
    "# Test pe cuvinte cheie\n",
    "if 'doctors' in word_to_idx and 'medicine' in word_to_idx:\n",
    "    idx1 = word_to_idx['doctors']\n",
    "    idx2 = word_to_idx['medicine']\n",
    "    sim = cosine_similarity(word_vectors_normalized[idx1], word_vectors_normalized[idx2])\n",
    "    print(f\"  Test similarity doctors-medicine: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1200b87c-c169-4cba-84e4-f163d4581e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ ANALOGIA 1: doctors - medicine + law = ?\n",
      "\n",
      "üî¨ SOLVING: doctors - medicine + law = ?\n",
      "üìä Top 10 candidates:\n",
      "  1. engineers    (0.5875)\n",
      "  2. compliance   (0.2951)\n",
      "  3. requires     (0.2759)\n",
      "  4. strictly     (0.2433)\n",
      "  5. and          (0.2433)\n",
      "  6. ensuring     (0.2340)\n",
      "  7. understand   (0.2145)\n",
      "  8. professional (0.2109)\n",
      "  9. helps        (0.2092)\n",
      "  10. effective    (0.2015)\n",
      "\n",
      "üéØ ANALOGIA 2: teachers - schools + hospitals = ?\n",
      "\n",
      "üî¨ SOLVING: teachers - schools + hospitals = ?\n",
      "üìä Top 10 candidates:\n",
      "  1. athletes     (0.3773)\n",
      "  2. center       (0.2877)\n",
      "  3. injuries     (0.2621)\n",
      "  4. recover      (0.2523)\n",
      "  5. help         (0.2372)\n",
      "  6. serve        (0.2300)\n",
      "  7. receiving    (0.2088)\n",
      "  8. as           (0.2007)\n",
      "  9. primary      (0.1959)\n",
      "  10. completely   (0.1954)\n",
      "\n",
      "üìã RAW RESULTS:\n",
      "  Analogy 1: engineers\n",
      "  Analogy 2: athletes\n"
     ]
    }
   ],
   "source": [
    "def solve_analogy_ppmi(word_a, word_b, word_c, topn=10):\n",
    "    \"\"\"RezolvƒÉ analogia A - B + C = D cu PPMI embeddings\"\"\"\n",
    "    \n",
    "    print(f\"\\nüî¨ SOLVING: {word_a} - {word_b} + {word_c} = ?\")\n",
    "    \n",
    "    # Verificare cuvinte √Æn vocabular\n",
    "    missing = []\n",
    "    for word in [word_a, word_b, word_c]:\n",
    "        if word not in word_to_idx:\n",
    "            missing.append(word)\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"‚ùå Missing words: {missing}\")\n",
    "        return []\n",
    "    \n",
    "    # Calculez vectorul »õintƒÉ: t = v(C) + (v(A) - v(B))\n",
    "    idx_a = word_to_idx[word_a]\n",
    "    idx_b = word_to_idx[word_b]\n",
    "    idx_c = word_to_idx[word_c]\n",
    "    \n",
    "    vec_a = word_vectors_normalized[idx_a]\n",
    "    vec_b = word_vectors_normalized[idx_b]\n",
    "    vec_c = word_vectors_normalized[idx_c]\n",
    "    \n",
    "    target_vector = vec_c + (vec_a - vec_b)\n",
    "    target_vector = target_vector / (np.linalg.norm(target_vector) + 1e-8)\n",
    "    \n",
    "    # Calculez similaritƒÉ»õi cu toate cuvintele\n",
    "    scores = []\n",
    "    exclude_words = {word_a, word_b, word_c}\n",
    "    \n",
    "    for word, idx in word_to_idx.items():\n",
    "        if word not in exclude_words:\n",
    "            vec = word_vectors_normalized[idx]\n",
    "            similarity = np.dot(vec, target_vector)\n",
    "            scores.append((word, similarity))\n",
    "    \n",
    "    # Sortez descrescƒÉtor\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"üìä Top {topn} candidates:\")\n",
    "    for i, (word, score) in enumerate(scores[:topn], 1):\n",
    "        print(f\"  {i}. {word:<12} ({score:.4f})\")\n",
    "    \n",
    "    return scores[:topn]\n",
    "\n",
    "# Rezolvare Analogia 1: doctors - medicine + law = ?\n",
    "print(\"üéØ ANALOGIA 1: doctors - medicine + law = ?\")\n",
    "results1 = solve_analogy_ppmi('doctors', 'medicine', 'law')\n",
    "answer1 = results1[0][0] if results1 else None\n",
    "\n",
    "# Rezolvare Analogia 2: teachers - schools + hospitals = ?\n",
    "print(f\"\\nüéØ ANALOGIA 2: teachers - schools + hospitals = ?\")\n",
    "results2 = solve_analogy_ppmi('teachers', 'schools', 'hospitals')\n",
    "answer2 = results2[0][0] if results2 else None\n",
    "\n",
    "print(f\"\\nüìã RAW RESULTS:\")\n",
    "print(f\"  Analogy 1: {answer1}\")\n",
    "print(f\"  Analogy 2: {answer2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d447a794-b61f-4879-8a3f-f3bc76d1e7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö© FLAG GENERATION\n",
      "=========================\n",
      "üìä CANONICALIZATION:\n",
      "  engineers ‚Üí ENGINEER\n",
      "  athletes ‚Üí ATHLETE\n",
      "\n",
      "üèÜ FINAL FLAG: SIGMOID_ENGINEER_ATHLETE\n",
      "\n",
      "‚úÖ QUALITY CHECKS:\n",
      "  ‚úÖ Format\n",
      "  ‚úÖ Uppercase\n",
      "  ‚úÖ No spaces\n",
      "  ‚úÖ Length\n",
      "\n",
      "üíæ Flag saved to final_flag.txt\n",
      "üéØ SUBMIT THIS FLAG: SIGMOID_ENGINEER_ATHLETE\n",
      "\n",
      "üéâ DETERMINISTIC SOLUTION COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "def canonicalize_word(word):\n",
    "    \"\"\"Mapare la formƒÉ canonicƒÉ pentru FLAG\"\"\"\n",
    "    \n",
    "    canonical_map = {\n",
    "        'doctors': 'DOCTOR',\n",
    "        'doctor': 'DOCTOR',\n",
    "        'engineers': 'ENGINEER', \n",
    "        'engineer': 'ENGINEER',\n",
    "        'teachers': 'TEACHER',\n",
    "        'teacher': 'TEACHER',\n",
    "        'athletes': 'ATHLETE',\n",
    "        'athlete': 'ATHLETE',\n",
    "        'lawyers': 'LAWYER',\n",
    "        'lawyer': 'LAWYER',\n",
    "        'nurses': 'NURSE',\n",
    "        'nurse': 'NURSE'\n",
    "    }\n",
    "    \n",
    "    if word.lower() in canonical_map:\n",
    "        return canonical_map[word.lower()]\n",
    "    \n",
    "    # Fallback: eliminƒÉ 's' final »ôi uppercase\n",
    "    if word.endswith('s') and len(word) > 3:\n",
    "        return word[:-1].upper()\n",
    "    \n",
    "    return word.upper()\n",
    "\n",
    "print(\"üö© FLAG GENERATION\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "if answer1 and answer2:\n",
    "    # Canonicalizare\n",
    "    canonical1 = canonicalize_word(answer1)\n",
    "    canonical2 = canonicalize_word(answer2)\n",
    "    \n",
    "    # Construire FLAG\n",
    "    flag = f\"SIGMOID_{canonical1}_{canonical2}\"\n",
    "    \n",
    "    print(f\"üìä CANONICALIZATION:\")\n",
    "    print(f\"  {answer1} ‚Üí {canonical1}\")\n",
    "    print(f\"  {answer2} ‚Üí {canonical2}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ FINAL FLAG: {flag}\")\n",
    "    \n",
    "    # VerificƒÉri finale\n",
    "    checks = {\n",
    "        'Format': flag.startswith('SIGMOID_') and flag.count('_') == 2,\n",
    "        'Uppercase': flag == flag.upper(),\n",
    "        'No spaces': ' ' not in flag,\n",
    "        'Length': 10 <= len(flag) <= 25\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úÖ QUALITY CHECKS:\")\n",
    "    for check, passed in checks.items():\n",
    "        status = \"‚úÖ\" if passed else \"‚ùå\"\n",
    "        print(f\"  {status} {check}\")\n",
    "    \n",
    "    # Salvare\n",
    "    with open('final_flag.txt', 'w') as f:\n",
    "        f.write(flag)\n",
    "    \n",
    "    print(f\"\\nüíæ Flag saved to final_flag.txt\")\n",
    "    print(f\"üéØ SUBMIT THIS FLAG: {flag}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Failed to solve analogies\")\n",
    "\n",
    "print(f\"\\nüéâ DETERMINISTIC SOLUTION COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60267ee-a5c1-4e57-9421-8096c4bf6582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
